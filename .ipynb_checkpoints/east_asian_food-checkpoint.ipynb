{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Grocery Store should I go for East Asian Food? Logit Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep # control the crawl rate to avoid hammering the servers with too many requests\n",
    "from random import randint\n",
    "import re # regular expression\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary # adds chromedriver binary to path\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer #tokenizing the words\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_pages(list_of_pages):\n",
    "    pages = []\n",
    "    for page in list_of_pages:\n",
    "        if pd.notnull(page):\n",
    "            all_pages = requests.get(page)\n",
    "            each_page = BeautifulSoup(all_pages.content, \"html.parser\")\n",
    "            pages.append(each_page)\n",
    "            sleep(randint(2,6))\n",
    "        else: \n",
    "            pages.append(None)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping H Mart Items for East Asian Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmart = ['https://www.hmart.com/groceries?p=' + str(i) for i in range(1,16)]\n",
    "hmart_parsed = parsing_pages(hmart)\n",
    "\n",
    "# finding item names from a list of beautifulsoup objects\n",
    "key = 'product name product-item-name'\n",
    "items = [] \n",
    "for page in hmart_parsed:\n",
    "    items_in_pages = page.find_all('strong', class_=key)\n",
    "    for item in items_in_pages: # cleaning up the messy text\n",
    "        items_per_page = item.get_text().replace('\\n','').replace('\\r','').strip().replace('     ', ' ').lower()\n",
    "        items.append(items_per_page)\n",
    "\n",
    "# extracting item names from the text\n",
    "item_names = [i.split('        ')[-1] for i in items]\n",
    "\n",
    "# extracting volumns at the end\n",
    "regrex = re.compile(r'\\d\\..*')\n",
    "item_volumns = [regrex.findall(i) for i in item_names]\n",
    "\n",
    "# getting rid of texts about volumns \n",
    "asian_items_no_volumns = []\n",
    "for i in asian_items:\n",
    "    asian_item = re.sub(' \\d\\..*', '', i)\n",
    "    asian_items_no_volumns.append(asian_item)\n",
    "\n",
    "columns = {\n",
    "    'item_name': asian_items_no_volumns,\n",
    "    'item_volumn': item_volumns,\n",
    "    'type': 'asian'\n",
    "}\n",
    "asian_items = pd.DataFrame(data = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Walmart Items for Non-East Asian Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Walmart uses dynamic web pages, thus can't use BeautifulSoup\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.walmart.com/browse/food/\")\n",
    "\n",
    "list_of_items_from_pages = []\n",
    "for n in range(1,26):\n",
    "        driver.get(\"https://www.walmart.com/browse/food/?page=\" + str(n))\n",
    "        items_from_pages = driver.find_elements_by_css_selector(\".product-title-link.line-clamp span\")\n",
    "        for i in items_from_pages:\n",
    "            items_extracted = i.text\n",
    "            list_of_items_from_pages.append(items_extracted)\n",
    "        time.sleep(3)\n",
    "\n",
    "        \n",
    "# getting rid of units/volumns at the beginning of each item\n",
    "pack = re.compile(r'\\(\\d{1,} (?:pack|count|cans)\\)') \n",
    "list_of_items_from_pages = [re.sub(pack, '', i.lower()) for i in list_of_items_from_pages]\n",
    "\n",
    "# isolating volumns at the end\n",
    "regrex = re.compile(r'(?:, \\d{1,}|\\d{1,} (?:mg|oz|fl|ct)|, \\d{1,} (?:mg|oz|fl|ct)).*')\n",
    "walmart_item_volumns = [regrex.findall(i) for i in list_of_items_from_pages]\n",
    "walmart_item_names = [re.sub(regrex, '', i.split(',')[0].strip()) for i in list_of_items_from_pages]\n",
    "\n",
    "columns = {\n",
    "    'item_name': walmart_item_names,\n",
    "    'item_volumn': walmart_item_volumns,\n",
    "    'type': 'non-asian'\n",
    "}\n",
    "walmart_items = pd.DataFrame(data = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, concat the two lists together\n",
    "frames = [walmart_items, hmart]\n",
    "df = pd.concat(frames).reset_index(drop = True)\n",
    "\n",
    "# seperate training and test sets\n",
    "from random import sample\n",
    "df_train = df.sample(frac=0.6, random_state=0)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english',max_df=0.85)\n",
    "train_vect = count_vect.fit_transform(df_train.item_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing/vectorizing the Item Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "train_tf_transformer = TfidfTransformer(use_idf=False).fit(train_vect)\n",
    "train_tf = train_tf_transformer.transform(train_vect)\n",
    "train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "test_vect = count_vect.transform(df_test.item_name) # not fitting, but transforming\n",
    "# test_tf_transformer = TfidfTransformer(use_idf=False).fit(train_vect)\n",
    "test_tf = train_tf_transformer.transform(test_vect)\n",
    "test_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_tf\n",
    "y_train = df_train.type\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "X_test = test_tf\n",
    "y_test = df_test.type\n",
    "\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Wegmans and Harris Teeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "wegmans_international_items = []\n",
    "driver.get(\"https://shop.wegmans.com/shop/categories/345?page=1\")\n",
    "time.sleep(3)\n",
    "driver.find_element_by_css_selector(\"#shopping-selector-shop-context-intent-instore\").click()\n",
    "time.sleep(3)\n",
    "for n in range(1,26):\n",
    "    time.sleep(2)\n",
    "    driver.get(\"https://shop.wegmans.com/shop/categories/345?page=\" + str(n))\n",
    "    time.sleep(3)\n",
    "    items = driver.find_elements_by_css_selector(\".cell-title\")\n",
    "    for i in items:\n",
    "        items_extracted = i.text\n",
    "        wegmans_international_items.append(items_extracted)\n",
    "    time.sleep(randint(3,10))\n",
    "\n",
    "wegmans_intl_item_names = [i.split('\\n')[0] for i in wegmans_international_items]\n",
    "wegmans_intl_item_volumns = [i.split('\\n')[1] for i in wegmans_international_items]\n",
    "\n",
    "columns = {\n",
    "    'item_name': wegmans_intl_item_names,\n",
    "    'item_volumn': wegmans_intl_item_volumns\n",
    "}\n",
    "wegmans_intl = pd.DataFrame(data = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = round(len(y_intl[y_intl == 'asian']) / len(y_intl), 2) \n",
    "print('Proportion of East Asian Food: ', rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo the identified food items\n",
    "wegmand_intl['type'] = y_intl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
